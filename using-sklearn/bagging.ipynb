{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation, preprocessing, decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, fbeta_score, recall_score, precision_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "receipts = [\n",
    "    'MED_CM_TBL_2016',\n",
    "    'MED_CO_TBL_2016',\n",
    "    'MED_GR_TBL_2016',\n",
    "    'MED_HOKO_TBL_2016',\n",
    "    'MED_IY_TBL_2016',\n",
    "    'MED_SI_TBL_2016',\n",
    "    'MED_SJ_TBL_2016',\n",
    "    'MED_SY_TBL_2016',\n",
    "    'MED_TO_TBL_2016',\n",
    "    'PHA_CM_TBL_2016',\n",
    "    'PHA_CO_TBL_2016',\n",
    "    'PHA_HOKO_TBL_2016',\n",
    "    'PHA_IY_TBL_2016',\n",
    "    'PHA_TO_TBL_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client.kikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'kikin.sqlite3'\n",
    "con = sqlite3.connect(dbname)\n",
    "c = con.cursor()\n",
    "dfs = {}\n",
    "for receipt in receipts:\n",
    "    q = 'select * from {}'.format(receipt)\n",
    "    df = pd.io.sql.read_sql(q,con)\n",
    "    dfs[receipt] = df\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病、診療、医薬から説明変数を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2930"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SY_TBL_2016']\n",
    "sy = df['shobyo_code'].drop_duplicates()\n",
    "n_sy = df['shobyo_code'].drop_duplicates().count()\n",
    "n_sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SI_TBL_2016']\n",
    "si = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_si = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_IY_TBL_2016']\n",
    "iy = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_iy = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_iy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.concat([sy,iy,si]).dropna()\n",
    "dd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病コード、診療コード、医薬コードに重複なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_values = dd.values#.tolist()\n",
    "x_size = x_values.size\n",
    "x_dic = { v:k for (k,v) in enumerate(x_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 素性をつくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "doc = db.med.find()\n",
    "for row in doc:\n",
    "    x = np.zeros(x_size)\n",
    "    \n",
    "    # 該当する傷病があれば、その傷病に対応するindexの値を1, なければ0\n",
    "    if 'MED_SY_TBL_2016' in row:\n",
    "        for d in row['MED_SY_TBL_2016']:\n",
    "            idx = x_dic[d['shobyo_code']]\n",
    "            x[idx] = 1\n",
    "            \n",
    "    # 医薬品に対応するindexに点数を挿入\n",
    "    if 'MED_IY_TBL_2016' in row:\n",
    "        for d in row['MED_IY_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "    # 診療に対応するindexに点数を挿入\n",
    "    if 'MED_SI_TBL_2016' in row:\n",
    "        for d in row['MED_SI_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "\n",
    "    # 保険者レコードか公費レコード、どちらかに請求点数と決定点数に差があれば異常とし1\n",
    "    diff =  sum([d['diff_tensu'] for d in row['MED_HOKO_TBL_2016']])\n",
    "    y = 1 if diff else 0\n",
    "    \n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "x_data = np.array(xs)\n",
    "y_data = np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAのち、近傍法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling\n",
      "f1 score Train: 0.991742658146\n",
      "f1 score Test: 0.362068965517\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.477272727273\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 0.97962371317\n",
      "f1 score Test: 0.254901960784\n",
      "recall score Train: 0.998986388648\n",
      "recall score Test: 0.590909090909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5))])\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.254901960784\n",
      "recall: 0.590909090909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2822,  134],\n",
       "        [  18,   26]]), 2822, 134, 18, 26)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pipe_gb.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A).ravel()\n",
    "print('f1:',f1_score(y_test, A))\n",
    "print('recall:',recall_score(y_test, A))\n",
    "confusion_matrix(y_test, A),tn, fp, fn, tp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
