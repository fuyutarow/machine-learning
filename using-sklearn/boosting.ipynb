{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation, preprocessing, decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "receipts = [\n",
    "    'MED_CM_TBL_2016',\n",
    "    'MED_CO_TBL_2016',\n",
    "    'MED_GR_TBL_2016',\n",
    "    'MED_HOKO_TBL_2016',\n",
    "    'MED_IY_TBL_2016',\n",
    "    'MED_SI_TBL_2016',\n",
    "    'MED_SJ_TBL_2016',\n",
    "    'MED_SY_TBL_2016',\n",
    "    'MED_TO_TBL_2016',\n",
    "    'PHA_CM_TBL_2016',\n",
    "    'PHA_CO_TBL_2016',\n",
    "    'PHA_HOKO_TBL_2016',\n",
    "    'PHA_IY_TBL_2016',\n",
    "    'PHA_TO_TBL_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client.kikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'kikin.sqlite3'\n",
    "con = sqlite3.connect(dbname)\n",
    "c = con.cursor()\n",
    "dfs = {}\n",
    "for receipt in receipts:\n",
    "    q = 'select * from {}'.format(receipt)\n",
    "    df = pd.io.sql.read_sql(q,con)\n",
    "    dfs[receipt] = df\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病、診療、医薬から説明変数を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2930"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SY_TBL_2016']\n",
    "sy = df['shobyo_code'].drop_duplicates()\n",
    "n_sy = df['shobyo_code'].drop_duplicates().count()\n",
    "n_sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SI_TBL_2016']\n",
    "si = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_si = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_IY_TBL_2016']\n",
    "iy = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_iy = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_iy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.concat([sy,iy,si]).dropna()\n",
    "dd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病コード、診療コード、医薬コードに重複なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_values = dd.values#.tolist()\n",
    "x_size = x_values.size\n",
    "x_dic = { v:k for (k,v) in enumerate(x_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 素性をつくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "doc = db.med.find()\n",
    "for row in doc:\n",
    "    x = np.zeros(x_size)\n",
    "    \n",
    "    # 該当する傷病があれば、その傷病に対応するindexの値を1, なければ0\n",
    "    if 'MED_SY_TBL_2016' in row:\n",
    "        for d in row['MED_SY_TBL_2016']:\n",
    "            idx = x_dic[d['shobyo_code']]\n",
    "            x[idx] = 1\n",
    "            \n",
    "    # 医薬品に対応するindexに点数を挿入\n",
    "    if 'MED_IY_TBL_2016' in row:\n",
    "        for d in row['MED_IY_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "    # 診療に対応するindexに点数を挿入\n",
    "    if 'MED_SI_TBL_2016' in row:\n",
    "        for d in row['MED_SI_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "\n",
    "    # 保険者レコードか公費レコード、どちらかに請求点数と決定点数に差があれば異常として1\n",
    "    diff =  sum([d['diff_tensu'] for d in row['MED_HOKO_TBL_2016']])\n",
    "    y = 1 if diff else 0\n",
    "    \n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "x_data = np.array(xs)\n",
    "y_data = np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAのち、ブースティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling\n",
      "f1 score Train: 0.9925265881\n",
      "f1 score Test: 0.276923076923\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.409090909091\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 0.978028249394\n",
      "f1 score Test: 0.194444444444\n",
      "recall score Train: 0.992615117289\n",
      "recall score Test: 0.477272727273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',GradientBoostingClassifier(random_state=1))])\n",
    "\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.477272727273\n",
      "0.194444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2805,  151],\n",
       "        [  23,   21]]), 2805, 151, 23, 21)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = pipe_gb.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,xx).ravel()\n",
    "r = recall_score(y_test,xx)\n",
    "f = f1_score(y_test,xx)\n",
    "print(r)\n",
    "print(f)\n",
    "confusion_matrix(y_test,xx),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10457516339869281, 0.10457516339869281)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/(tp+fp), precision_score(y_test,xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2819,  137],\n",
       "       [  28,   16]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = pipe_gb.predict(X_test)\n",
    "confusion_matrix(y_test, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):\n",
      "    \"\"\"Compute confusion matrix to evaluate the accuracy of a classification\n",
      "\n",
      "    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "    is equal to the number of observations known to be in group :math:`i` but\n",
      "    predicted to be in group :math:`j`.\n",
      "\n",
      "    Thus in binary classification, the count of true negatives is\n",
      "    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array, shape = [n_samples]\n",
      "        Ground truth (correct) target values.\n",
      "\n",
      "    y_pred : array, shape = [n_samples]\n",
      "        Estimated targets as returned by a classifier.\n",
      "\n",
      "    labels : array, shape = [n_classes], optional\n",
      "        List of labels to index the matrix. This may be used to reorder\n",
      "        or select a subset of labels.\n",
      "        If none is given, those that appear at least once\n",
      "        in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "\n",
      "    sample_weight : array-like of shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    C : array, shape = [n_classes, n_classes]\n",
      "        Confusion matrix\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Wikipedia entry for the Confusion matrix\n",
      "           <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import confusion_matrix\n",
      "    >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "    >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "    >>> confusion_matrix(y_true, y_pred)\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "\n",
      "    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "    >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "\n",
      "    In the binary case, we can extract true positives, etc as follows:\n",
      "\n",
      "    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "    >>> (tn, fp, fn, tp)\n",
      "    (0, 2, 1, 1)\n",
      "\n",
      "    \"\"\"\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "    if y_type not in (\"binary\", \"multiclass\"):\n",
      "        raise ValueError(\"%s is not supported\" % y_type)\n",
      "\n",
      "    if labels is None:\n",
      "        labels = unique_labels(y_true, y_pred)\n",
      "    else:\n",
      "        labels = np.asarray(labels)\n",
      "        if np.all([l not in y_true for l in labels]):\n",
      "            raise ValueError(\"At least one label specified must be in y_true\")\n",
      "\n",
      "    if sample_weight is None:\n",
      "        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
      "    else:\n",
      "        sample_weight = np.asarray(sample_weight)\n",
      "\n",
      "    check_consistent_length(sample_weight, y_true, y_pred)\n",
      "\n",
      "    n_labels = labels.size\n",
      "    label_to_ind = dict((y, x) for x, y in enumerate(labels))\n",
      "    # convert yt, yp into index\n",
      "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
      "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
      "\n",
      "    # intersect y_pred, y_true with labels, eliminate items not in labels\n",
      "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
      "    y_pred = y_pred[ind]\n",
      "    y_true = y_true[ind]\n",
      "    # also eliminate weights of eliminated items\n",
      "    sample_weight = sample_weight[ind]\n",
      "\n",
      "    # Choose the accumulator dtype to always have high precision\n",
      "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
      "        dtype = np.int64\n",
      "    else:\n",
      "        dtype = np.float64\n",
      "\n",
      "    CM = coo_matrix((sample_weight, (y_true, y_pred)),\n",
      "                    shape=(n_labels, n_labels), dtype=dtype,\n",
      "                    ).toarray()\n",
      "\n",
      "    return CM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99120028159098905"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95263870094722602"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "f1 score Train: 0.99101599591\n",
      "f1 score Test: 0.979996580612\n",
      "recall score Train: 0.982334202143\n",
      "recall score Test: 0.969553450609\n",
      "\n",
      "Undersampling\n",
      "f1 score Train: 0.885572139303\n",
      "f1 score Test: 0.969075747047\n",
      "recall score Train: 0.946808510638\n",
      "recall score Test: 0.94350473613\n",
      "\n",
      "Oversampling\n",
      "f1 score Train: 0.997168372904\n",
      "f1 score Test: 0.988665200474\n",
      "recall score Train: 0.994352736751\n",
      "recall score Test: 0.98849797023\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 0.99101599591\n",
      "f1 score Test: 0.979996580612\n",
      "recall score Train: 0.982334202143\n",
      "recall score Test: 0.969553450609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),('est',KNeighborsClassifier(n_neighbors=3))])\n",
    "\n",
    "X_train, y_train = X_train, y_train\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Original')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_under, y_train_under\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Undersampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "f1 score Train: 0.964681233164\n",
      "f1 score Test: 0.960866526904\n",
      "recall score Train: 0.933391253982\n",
      "recall score Test: 0.930311231394\n",
      "\n",
      "Undersampling\n",
      "f1 score Train: 0.738095238095\n",
      "f1 score Test: 0.990518117169\n",
      "recall score Train: 0.989361702128\n",
      "recall score Test: 0.98951285521\n",
      "\n",
      "Oversampling\n",
      "f1 score Train: 0.980965028774\n",
      "f1 score Test: 0.976264189886\n",
      "recall score Train: 0.962641181581\n",
      "recall score Test: 0.960081190798\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 0.964681233164\n",
      "f1 score Test: 0.960866526904\n",
      "recall score Train: 0.933391253982\n",
      "recall score Test: 0.930311231394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),('est',KNeighborsClassifier(n_neighbors=20))])\n",
    "\n",
    "X_train, y_train = X_train, y_train\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Original')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_under, y_train_under\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Undersampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グリッドサーチする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc_param_grid = {\n",
    "    'n_neighbors': [1,3,5,10,20,30]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), svc_param_grid, cv=10)\n",
    "\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA200でk近傍法k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=200)\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "f1 score Train: 1.0\n",
      "f1 score Test: 0.991582491582\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.996278755074\n",
      "\n",
      "Undersampling\n",
      "f1 score Train: 1.0\n",
      "f1 score Test: 0.960770577933\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.927943166441\n",
      "\n",
      "Oversampling\n",
      "f1 score Train: 1.0\n",
      "f1 score Test: 0.991755005889\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.996955345061\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 1.0\n",
      "f1 score Test: 0.985600542097\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.984100135318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),('est',KNeighborsClassifier(n_neighbors=1))])\n",
    "\n",
    "X_train, y_train = X_train, y_train\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Original')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_under, y_train_under\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Undersampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6,   38],\n",
       "       [  47, 2909]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = pipe_gb.predict(X_test)\n",
    "confusion_matrix(y_test, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=1000)\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score Train: 0.0\n",
      "f1 score Test: 0.0\n",
      "recall score Train: 0.0\n",
      "recall score Test: 0.0\n",
      "\n",
      "Undersampling\n",
      "f1 score Train: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score Test: 0.0\n",
      "recall score Train: 0.0\n",
      "recall score Test: 0.0\n",
      "\n",
      "Oversampling\n",
      "f1 score Train: 0.99452764977\n",
      "f1 score Test: 0.112359550562\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.113636363636\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 0.986853386682\n",
      "f1 score Test: 0.14598540146\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.227272727273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),\n",
    "                    ('est',KNeighborsClassifier(n_neighbors=20))])\n",
    "\n",
    "X_train, y_train = X_train, y_train\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Original')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_under, y_train_under\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Undersampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxx = pipe_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [1, 7]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1, 0, 1, 1, 0, 1,1,1,1,1]\n",
    "y_pred = [0, 0, 1, 1, 0, 1,1,1,1,1]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2873,   83],\n",
       "       [  34,   10]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2873,   34],\n",
       "       [  83,   10]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(xxx, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * p * r / (p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, nb_class)\n",
    "y_test = np_utils.to_categorical(y_test, nb_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
