{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation, preprocessing, decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "receipts = [\n",
    "    'MED_CM_TBL_2016',\n",
    "    'MED_CO_TBL_2016',\n",
    "    'MED_GR_TBL_2016',\n",
    "    'MED_HOKO_TBL_2016',\n",
    "    'MED_IY_TBL_2016',\n",
    "    'MED_SI_TBL_2016',\n",
    "    'MED_SJ_TBL_2016',\n",
    "    'MED_SY_TBL_2016',\n",
    "    'MED_TO_TBL_2016',\n",
    "    'PHA_CM_TBL_2016',\n",
    "    'PHA_CO_TBL_2016',\n",
    "    'PHA_HOKO_TBL_2016',\n",
    "    'PHA_IY_TBL_2016',\n",
    "    'PHA_TO_TBL_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client.kikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'kikin.sqlite3'\n",
    "con = sqlite3.connect(dbname)\n",
    "c = con.cursor()\n",
    "dfs = {}\n",
    "for receipt in receipts:\n",
    "    q = 'select * from {}'.format(receipt)\n",
    "    df = pd.io.sql.read_sql(q,con)\n",
    "    dfs[receipt] = df\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病、診療、医薬から説明変数を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2930"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SY_TBL_2016']\n",
    "sy = df['shobyo_code'].drop_duplicates()\n",
    "n_sy = df['shobyo_code'].drop_duplicates().count()\n",
    "n_sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SI_TBL_2016']\n",
    "si = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_si = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_IY_TBL_2016']\n",
    "iy = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_iy = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_iy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7211"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.concat([sy,iy,si])\n",
    "len(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病コード、診療コード、医薬コードに重複なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_values = dd.values#.tolist()\n",
    "x_size = x_values.size\n",
    "x_dic = { v:k for (k,v) in enumerate(x_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 素性をつくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "doc = db.med.find()\n",
    "for row in doc:\n",
    "    x = np.zeros(x_size)\n",
    "    \n",
    "    # 該当する傷病があれば、その傷病に対応するindexの値を1, なければ0\n",
    "    if 'MED_SY_TBL_2016' in row:\n",
    "        for d in row['MED_SY_TBL_2016']:\n",
    "            idx = x_dic[d['shobyo_code']]\n",
    "            x[idx] = 1\n",
    "            \n",
    "    # 医薬品に対応するindexに点数を挿入\n",
    "    if 'MED_IY_TBL_2016' in row:\n",
    "        for d in row['MED_IY_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "    # 診療に対応するindexに点数を挿入\n",
    "    if 'MED_SI_TBL_2016' in row:\n",
    "        for d in row['MED_SI_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "\n",
    "    # 保険者レコードか公費レコード、どちらかに請求点数と決定点数に差があれば異常として1\n",
    "    diff =  sum([d['diff_tensu'] for d in row['MED_HOKO_TBL_2016']])\n",
    "    y = 1 if diff else 0\n",
    "    \n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "x_data = np.array(xs)\n",
    "y_data = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAで次元圧縮、のちSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling\n",
      "f1 score Train: 0.894346617238\n",
      "f1 score Test: 0.196531791908\n",
      "recall score Train: 0.838401390096\n",
      "recall score Test: 0.386363636364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),('est',SVC())])\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.196531791908\n",
      "recall: 0.386363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2844,  112],\n",
       "        [  27,   17]]), 2844, 112, 27, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pipe_gb.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A).ravel()\n",
    "print('f1:',f1_score(y_test, A))\n",
    "print('recall:',recall_score(y_test, A))\n",
    "confusion_matrix(y_test, A),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "f1 score Train: 0.875330327996\n",
      "f1 score Test: 0.108910891089\n",
      "recall score Train: 0.815377932233\n",
      "recall score Test: 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.108910891089\n",
      "recall: 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2809,  147],\n",
       "        [  33,   11]]), 2809, 147, 33, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pipe_gb.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A).ravel()\n",
    "print('f1:',f1_score(y_test, A))\n",
    "print('recall:',recall_score(y_test, A))\n",
    "confusion_matrix(y_test, A),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAをしないで、SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "f1 score Train: 0.919540229885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score Test: 0.0\n",
      "recall score Train: 0.851063829787\n",
      "recall score Test: 0.0\n",
      "\n",
      "Undersampling\n",
      "f1 score Train: 0.909090909091\n",
      "f1 score Test: 0.0615384615385\n",
      "recall score Train: 0.851063829787\n",
      "recall score Test: 0.909090909091\n",
      "\n",
      "Oversampling\n",
      "f1 score Train: 0.998698481562\n",
      "f1 score Test: 0.0344827586207\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.0227272727273\n",
      "\n",
      "SMOTE\n",
      "f1 score Train: 0.999348817018\n",
      "f1 score Test: 0.0363636363636\n",
      "recall score Train: 1.0\n",
      "recall score Test: 0.0227272727273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_gb = Pipeline([('scl',StandardScaler()),('est',SVC())])\n",
    "\n",
    "X_train, y_train = X_train, y_train\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Original')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_under, y_train_under\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Undersampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_over, y_train_over\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('Oversampling')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グリッドサーチする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "svc_grid_search = GridSearchCV(SVC(), svc_param_grid, cv=10)\n",
    "\n",
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "svc_grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train score: {:.3f}'.format(svc_grid_search.score(X_train, y_train)))\n",
    "print('Test score: {:.3f}'.format(svc_grid_search.score(X_test, y_test)))\n",
    "print('Confusion matrix:\\n{}'.format(confusion_matrix(y_test, svc_grid_search.predict(X_test))))\n",
    "print('Best parameters: {}'.format(svc_grid_search.best_params_))\n",
    "print('Best estimator: {}'.format(svc_grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('SMOTE')\n",
    "print('f1 score Train:', f1_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('f1 score Test:', f1_score(y_test, pipe_gb.predict(X_test)))\n",
    "print('recall score Train:', recall_score(y_train, pipe_gb.predict(X_train)))\n",
    "print('recall score Test:', recall_score(y_test, pipe_gb.predict(X_test)))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
