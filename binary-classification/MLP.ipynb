{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation, preprocessing, decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, fbeta_score, recall_score, precision_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "receipts = [\n",
    "    'MED_CM_TBL_2016',\n",
    "    'MED_CO_TBL_2016',\n",
    "    'MED_GR_TBL_2016',\n",
    "    'MED_HOKO_TBL_2016',\n",
    "    'MED_IY_TBL_2016',\n",
    "    'MED_SI_TBL_2016',\n",
    "    'MED_SJ_TBL_2016',\n",
    "    'MED_SY_TBL_2016',\n",
    "    'MED_TO_TBL_2016',\n",
    "    'PHA_CM_TBL_2016',\n",
    "    'PHA_CO_TBL_2016',\n",
    "    'PHA_HOKO_TBL_2016',\n",
    "    'PHA_IY_TBL_2016',\n",
    "    'PHA_TO_TBL_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client.kikin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname = 'kikin.sqlite3'\n",
    "con = sqlite3.connect(dbname)\n",
    "c = con.cursor()\n",
    "dfs = {}\n",
    "for receipt in receipts:\n",
    "    q = 'select * from {}'.format(receipt)\n",
    "    df = pd.io.sql.read_sql(q,con)\n",
    "    dfs[receipt] = df\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 傷病、診療、医薬から説明変数を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2930"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SY_TBL_2016']\n",
    "sy = df['shobyo_code'].drop_duplicates()\n",
    "n_sy = df['shobyo_code'].drop_duplicates().count()\n",
    "n_sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_SI_TBL_2016']\n",
    "si = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_si = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfs['MED_IY_TBL_2016']\n",
    "iy = df['s_tekiyo_code'].drop_duplicates()\n",
    "n_iy = df['s_tekiyo_code'].drop_duplicates().count()\n",
    "n_iy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.concat([sy,iy,si]).dropna()\n",
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_values = dd.values#.tolist()\n",
    "x_size = x_values.size\n",
    "x_dic = { v:k for (k,v) in enumerate(x_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 素性をつくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "doc = db.med.find()\n",
    "for row in doc:\n",
    "    x = np.zeros(x_size)\n",
    "    \n",
    "    # 該当する傷病があれば、その傷病に対応するindexの値を1, なければ0\n",
    "    if 'MED_SY_TBL_2016' in row:\n",
    "        for d in row['MED_SY_TBL_2016']:\n",
    "            idx = x_dic[d['shobyo_code']]\n",
    "            x[idx] = 1\n",
    "            \n",
    "    # 医薬品に対応するindexに点数を挿入\n",
    "    if 'MED_IY_TBL_2016' in row:\n",
    "        for d in row['MED_IY_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "    # 診療に対応するindexに点数を挿入\n",
    "    if 'MED_SI_TBL_2016' in row:\n",
    "        for d in row['MED_SI_TBL_2016']:\n",
    "            s_code = d['s_tekiyo_code']\n",
    "            k_code = d['k_tekiyo_code']\n",
    "            if math.isnan(s_code) and math.isnan(k_code):\n",
    "                continue\n",
    "            \n",
    "            code = s_code if not math.isnan(s_code) else k_code\n",
    "            idx = x_dic[code]\n",
    "            s = d['s_tensu']\n",
    "            k = d['k_tensu']\n",
    "            x[idx] =\\\n",
    "                s if not math.isnan(s) else\\\n",
    "                k if not math.isnan(k) else\\\n",
    "                0\n",
    "    \n",
    "\n",
    "    # 保険者レコードか公費レコード、どちらかに請求点数と決定点数に差があれば異常として0, 正常な値に1\n",
    "    diff =  sum([d['diff_tensu'] for d in row['MED_HOKO_TBL_2016']])\n",
    "    y = 1 if diff else 0\n",
    "    \n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "x_data = np.array(xs)\n",
    "y_data = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * p * r / (p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAのち、MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= preprocessing.StandardScaler()\n",
    "sc.fit(x_data)\n",
    "X = sc.transform(x_data)\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_train_under = to_categorical(y_train_under)\n",
    "Y_train_over = to_categorical(y_train_over)\n",
    "Y_train_smote = to_categorical(y_train_smote)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 1s - loss: 0.0993 - acc: 0.9799 - f1: 0.9799 - recall: 0.9799 - val_loss: 0.2830 - val_acc: 0.9610 - val_f1: 0.9610 - val_recall: 0.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ead999e80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98007153806847203"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97327469553450607"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   1.00000000e+00],\n",
       "       [  5.79111893e-21,   1.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00],\n",
       "       ..., \n",
       "       [  2.06120245e-30,   1.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,   44],\n",
       "       [   0, 2956]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, np.argmax(,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAなしでMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_train_under = to_categorical(y_train_under)\n",
    "Y_train_over = to_categorical(y_train_over)\n",
    "Y_train_smote = to_categorical(y_train_smote)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/6\n",
      "13812/13812 [==============================] - 67s - loss: 0.1223 - acc: 0.9642 - f1: 0.9642 - recall: 0.9642 - val_loss: 0.1001 - val_acc: 0.9620 - val_f1: 0.9620 - val_recall: 0.9620\n",
      "Epoch 2/6\n",
      "13812/13812 [==============================] - 63s - loss: 0.0589 - acc: 0.9823 - f1: 0.9823 - recall: 0.9823 - val_loss: 0.0869 - val_acc: 0.9680 - val_f1: 0.9680 - val_recall: 0.9680\n",
      "Epoch 3/6\n",
      "13812/13812 [==============================] - 64s - loss: 0.0523 - acc: 0.9838 - f1: 0.9838 - recall: 0.9838 - val_loss: 0.1196 - val_acc: 0.9553 - val_f1: 0.9553 - val_recall: 0.9553\n",
      "Epoch 4/6\n",
      "13812/13812 [==============================] - 63s - loss: 0.0550 - acc: 0.9810 - f1: 0.9810 - recall: 0.9810 - val_loss: 0.0852 - val_acc: 0.9620 - val_f1: 0.9620 - val_recall: 0.9620\n",
      "Epoch 5/6\n",
      "13812/13812 [==============================] - 63s - loss: 0.0525 - acc: 0.9801 - f1: 0.9801 - recall: 0.9801 - val_loss: 0.0999 - val_acc: 0.9563 - val_f1: 0.9563 - val_recall: 0.9563\n",
      "Epoch 6/6\n",
      "13812/13812 [==============================] - 64s - loss: 0.0479 - acc: 0.9828 - f1: 0.9828 - recall: 0.9828 - val_loss: 0.0837 - val_acc: 0.9717 - val_f1: 0.9717 - val_recall: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18c0f1aac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1200, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=6,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.308943089431\n",
      "recall: 0.431818181818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2896,   60],\n",
       "        [  25,   19]]), 2896, 60, 25, 19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 20s - loss: 1.8314 - acc: 0.8768 - f1: 0.8768 - recall: 0.8768 - val_loss: 1.9109 - val_acc: 0.8767 - val_f1: 0.8767 - val_recall: 0.8767\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 17s - loss: 2.0421 - acc: 0.8711 - f1: 0.8711 - recall: 0.8711 - val_loss: 2.0406 - val_acc: 0.8727 - val_f1: 0.8727 - val_recall: 0.8727\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 16s - loss: 2.6715 - acc: 0.8333 - f1: 0.8333 - recall: 0.8333 - val_loss: 0.2955 - val_acc: 0.9810 - val_f1: 0.9810 - val_recall: 0.9810\n",
      "\n",
      "f1: 0.0338983050847\n",
      "recall: 0.0227272727273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2942,   14],\n",
       "        [  43,    1]]), 2942, 14, 43, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=n_in, activation='elu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 18s - loss: 1.9065 - acc: 0.8724 - f1: 0.8724 - recall: 0.8724 - val_loss: 1.8967 - val_acc: 0.8790 - val_f1: 0.8790 - val_recall: 0.8790\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 16s - loss: 1.2829 - acc: 0.9187 - f1: 0.9187 - recall: 0.9187 - val_loss: 1.6974 - val_acc: 0.8930 - val_f1: 0.8930 - val_recall: 0.8930\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 18s - loss: 1.9245 - acc: 0.8788 - f1: 0.8788 - recall: 0.8788 - val_loss: 2.1074 - val_acc: 0.8677 - val_f1: 0.8677 - val_recall: 0.8677 ETA: 3s - loss - ETA: 0s - loss: 1.9450 - acc: 0.8775 - f1: 0.8775 - recal\n",
      "\n",
      "f1: 0.149892933619\n",
      "recall: 0.795454545455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2568,  388],\n",
       "        [   9,   35]]), 2568, 388, 9, 35)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 29s - loss: 0.4047 - acc: 0.9747 - f1: 0.9747 - recall: 0.9747 - val_loss: 0.5593 - val_acc: 0.9643 - val_f1: 0.9643 - val_recall: 0.9643\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 27s - loss: 0.3799 - acc: 0.9759 - f1: 0.9759 - recall: 0.9759 - val_loss: 0.6214 - val_acc: 0.9603 - val_f1: 0.9603 - val_recall: 0.9603\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 28s - loss: 0.3606 - acc: 0.9776 - f1: 0.9776 - recall: 0.9776 - val_loss: 0.6211 - val_acc: 0.9603 - val_f1: 0.9603 - val_recall: 0.9603 - recall: 0 - ETA: 3s - los\n",
      "\n",
      "f1: 0.304093567251\n",
      "recall: 0.590909090909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2855,  101],\n",
       "        [  18,   26]]), 2855, 101, 18, 26)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('f0.3.elu.adam.model.h5')\n",
    "model.save_weights('f0.3.elu.adam.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 17s - loss: 2.4417 - acc: 0.8402 - f1: 0.8402 - recall: 0.8402 - val_loss: 2.5600 - val_acc: 0.8387 - val_f1: 0.8387 - val_recall: 0.83871 - ETA: 2s - loss: 2.6000\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 14s - loss: 2.0429 - acc: 0.8703 - f1: 0.8703 - recall: 0.8703 - val_loss: 2.9811 - val_acc: 0.8137 - val_f1: 0.8137 - val_recall: 0.8137\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 14s - loss: 1.5276 - acc: 0.9034 - f1: 0.9034 - recall: 0.9034 - val_loss: 2.6729 - val_acc: 0.8320 - val_f1: 0.8320 - val_recall: 0.8320\n",
      "\n",
      "f1: 0.134020618557\n",
      "recall: 0.886363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2457,  499],\n",
       "        [   5,   39]]), 2457, 499, 5, 39)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=n_in, activation='elu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 14s - loss: 1.4421 - acc: 0.9099 - f1: 0.9099 - recall: 0.9099 - val_loss: 2.6509 - val_acc: 0.8337 - val_f1: 0.8337 - val_recall: 0.8337\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 16s - loss: 1.3879 - acc: 0.9132 - f1: 0.9132 - recall: 0.9132 - val_loss: 2.5583 - val_acc: 0.8390 - val_f1: 0.8390 - val_recall: 0.8390\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 16s - loss: 1.2872 - acc: 0.9192 - f1: 0.9192 - recall: 0.9192 - val_loss: 2.3367 - val_acc: 0.8540 - val_f1: 0.8540 - val_recall: 0.8540\n",
      "\n",
      "f1: 0.130952380952\n",
      "recall: 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2529,  427],\n",
       "        [  11,   33]]), 2529, 427, 11, 33)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 10s - loss: 0.1527 - acc: 0.9458 - f1: 0.9458 - recall: 0.9458 - val_loss: 0.1130 - val_acc: 0.9643 - val_f1: 0.9643 - val_recall: 0.9643\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0419 - acc: 0.9878 - f1: 0.9878 - recall: 0.9878 - val_loss: 0.1020 - val_acc: 0.9740 - val_f1: 0.9740 - val_recall: 0.9740\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0353 - acc: 0.9905 - f1: 0.9905 - recall: 0.9905 - val_loss: 0.0988 - val_acc: 0.9740 - val_f1: 0.9740 - val_recall: 0.9740\n",
      "\n",
      "f1: 0.315789473684\n",
      "recall: 0.409090909091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2904,   52],\n",
       "        [  26,   18]]), 2904, 52, 26, 18)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 9s - loss: 0.1562 - acc: 0.9494 - f1: 0.9494 - recall: 0.9494 - val_loss: 0.1038 - val_acc: 0.9710 - val_f1: 0.9710 - val_recall: 0.9710\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0374 - acc: 0.9896 - f1: 0.9896 - recall: 0.9896 - val_loss: 0.1332 - val_acc: 0.9660 - val_f1: 0.9660 - val_recall: 0.9660\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0321 - acc: 0.9911 - f1: 0.9911 - recall: 0.9911 - val_loss: 0.1209 - val_acc: 0.9727 - val_f1: 0.9727 - val_recall: 0.9727\n",
      "\n",
      "f1: 0.338709677419\n",
      "recall: 0.477272727273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2897,   59],\n",
       "        [  23,   21]]), 2897, 59, 23, 21)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 27s - loss: 0.1021 - acc: 0.9655 - f1: nan - recall: 0.5357 - val_loss: 0.1080 - val_acc: 0.9620 - val_f1: 0.8971 - val_recall: 0.8213\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 25s - loss: 0.0500 - acc: 0.9853 - f1: 0.6504 - recall: 0.4947 - val_loss: 0.1198 - val_acc: 0.9647 - val_f1: 0.9064 - val_recall: 0.8360\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 25s - loss: 0.0377 - acc: 0.9893 - f1: nan - recall: 0.4542 - val_loss: 0.1152 - val_acc: 0.9690 - val_f1: 0.9326 - val_recall: 0.8803\n",
      "\n",
      "f1: 0.321167883212\n",
      "recall: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2885,   71],\n",
       "        [  22,   22]]), 2885, 71, 22, 22)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Input, concatenate\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "inputs = Input(shape=(n_in,))\n",
    "x = Dense(64, input_dim=n_in, activation='sigmoid')(inputs)\n",
    "x = Dense(64, activation='sigmoid')(concatenate([x,inputs]))\n",
    "x = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 33s - loss: 0.1026 - acc: 0.9689 - f1: 0.8051 - recall: 0.6952 - val_loss: 0.0951 - val_acc: 0.9707 - val_f1: 0.9426 - val_recall: 0.8987\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 31s - loss: 0.0421 - acc: 0.9878 - f1: 0.7639 - recall: 0.6282 - val_loss: 0.1117 - val_acc: 0.9720 - val_f1: 0.9530 - val_recall: 0.9167\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 32s - loss: 0.0345 - acc: 0.9904 - f1: 0.7165 - recall: 0.5707 - val_loss: 0.1136 - val_acc: 0.9727 - val_f1: 0.9511 - val_recall: 0.9147\n",
      "\n",
      "f1: 0.293103448276\n",
      "recall: 0.386363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2901,   55],\n",
       "        [  27,   17]]), 2901, 55, 27, 17)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Input, concatenate\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "inputs = Input(shape=(n_in,))\n",
    "sig = Dense(64, input_dim=n_in, activation='sigmoid')(inputs)\n",
    "relu = Dense(64, input_dim=n_in, activation='relu')(inputs)\n",
    "elu = Dense(64, input_dim=n_in, activation='elu')(inputs)\n",
    "x = Dense(64, activation='sigmoid')(concatenate([sig,relu,elu]))\n",
    "x = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 17s - loss: 0.0273 - acc: 0.9924 - f1: 0.7833 - recall: 0.6484 - val_loss: 0.1131 - val_acc: 0.9760 - val_f1: 0.9533 - val_recall: 0.9177\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 21s - loss: 0.0247 - acc: 0.9933 - f1: 0.8085 - recall: 0.6837 - val_loss: 0.1306 - val_acc: 0.9713 - val_f1: 0.9500 - val_recall: 0.9130\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 17s - loss: 0.0228 - acc: 0.9941 - f1: 0.8165 - recall: 0.6942 - val_loss: 0.1171 - val_acc: 0.9753 - val_f1: 0.9561 - val_recall: 0.9230\n",
      "\n",
      "f1: 0.350877192982\n",
      "recall: 0.454545454545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2906,   50],\n",
       "        [  24,   20]]), 2906, 50, 24, 20)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 18s - loss: 0.0246 - acc: 0.9925 - f1: 0.8491 - recall: 0.7422 - val_loss: 0.1136 - val_acc: 0.9747 - val_f1: 0.9490 - val_recall: 0.9103\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 17s - loss: 0.0225 - acc: 0.9945 - f1: 0.8623 - recall: 0.7622 - val_loss: 0.1158 - val_acc: 0.9760 - val_f1: 0.9515 - val_recall: 0.9157\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 17s - loss: 0.0191 - acc: 0.9959 - f1: 0.8814 - recall: 0.7914 - val_loss: 0.1121 - val_acc: 0.9777 - val_f1: 0.9546 - val_recall: 0.9213\n",
      "\n",
      "f1: 0.323232323232\n",
      "recall: 0.363636363636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2917,   39],\n",
       "        [  28,   16]]), 2917, 39, 28, 16)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 8s - loss: 0.1281 - acc: 0.9579 - f1: 0.9579 - recall: 0.9579 - val_loss: 0.0981 - val_acc: 0.9667 - val_f1: 0.9667 - val_recall: 0.9667\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0431 - acc: 0.9873 - f1: 0.9873 - recall: 0.9873 - val_loss: 0.1133 - val_acc: 0.9663 - val_f1: 0.9663 - val_recall: 0.9663\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0294 - acc: 0.9915 - f1: 0.9915 - recall: 0.9915 - val_loss: 0.1077 - val_acc: 0.9747 - val_f1: 0.9747 - val_recall: 0.9747\n",
      "\n",
      "f1: 0.309090909091\n",
      "recall: 0.386363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2907,   49],\n",
       "        [  27,   17]]), 2907, 49, 27, 17)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0181 - acc: 0.9954 - f1: 0.9954 - recall: 0.9954 - val_loss: 0.1413 - val_acc: 0.9747 - val_f1: 0.9747 - val_recall: 0.9747\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0175 - acc: 0.9959 - f1: 0.9959 - recall: 0.9959 - val_loss: 0.1419 - val_acc: 0.9787 - val_f1: 0.9787 - val_recall: 0.9787\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 7s - loss: 0.0168 - acc: 0.9958 - f1: 0.9958 - recall: 0.9958 - val_loss: 0.1517 - val_acc: 0.9757 - val_f1: 0.9757 - val_recall: 0.9757\n",
      "\n",
      "f1: 0.291262135922\n",
      "recall: 0.340909090909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2912,   44],\n",
       "        [  29,   15]]), 2912, 44, 29, 15)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 6s - loss: 0.1935 - acc: 0.9299 - f1: 0.9299 - recall: 0.9299 - val_loss: 0.1390 - val_acc: 0.9383 - val_f1: 0.9383 - val_recall: 0.9383\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 6s - loss: 0.1158 - acc: 0.9647 - f1: 0.9647 - recall: 0.9647 - val_loss: 0.1461 - val_acc: 0.9420 - val_f1: 0.9420 - val_recall: 0.9420\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 5s - loss: 0.1079 - acc: 0.9701 - f1: 0.9701 - recall: 0.9701 - val_loss: 0.1545 - val_acc: 0.9380 - val_f1: 0.9380 - val_recall: 0.9380\n",
      "\n",
      "f1: 0.225\n",
      "recall: 0.613636363636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2787,  169],\n",
       "        [  17,   27]]), 2787, 169, 17, 27)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, input_dim=n_in))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 10s - loss: 0.1871 - acc: 0.9364 - f1: 0.9364 - recall: 0.9364 - val_loss: 0.1644 - val_acc: 0.9387 - val_f1: 0.9387 - val_recall: 0.9387\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 9s - loss: 0.0974 - acc: 0.9726 - f1: 0.9726 - recall: 0.9726 - val_loss: 0.1453 - val_acc: 0.9420 - val_f1: 0.9420 - val_recall: 0.9420\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 10s - loss: 0.0869 - acc: 0.9753 - f1: 0.9753 - recall: 0.9753 - val_loss: 0.1484 - val_acc: 0.9453 - val_f1: 0.9453 - val_recall: 0.9453\n",
      "\n",
      "f1: 0.219047619048\n",
      "recall: 0.522727272727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2813,  143],\n",
       "        [  21,   23]]), 2813, 143, 21, 23)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 10s - loss: 1.9533 - acc: 0.8700 - f1: 0.8700 - recall: 0.8700 - val_loss: 2.2653 - val_acc: 0.8523 - val_f1: 0.8523 - val_recall: 0.8523\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 9s - loss: 1.3783 - acc: 0.9117 - f1: 0.9117 - recall: 0.9117 - val_loss: 1.5496 - val_acc: 0.9010 - val_f1: 0.9010 - val_recall: 0.9010\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 9s - loss: 1.1215 - acc: 0.9289 - f1: 0.9289 - recall: 0.9289 - val_loss: 1.6942 - val_acc: 0.8923 - val_f1: 0.8923 - val_recall: 0.8923\n",
      "\n",
      "f1: 0.165374677003\n",
      "recall: 0.727272727273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2645,  311],\n",
       "        [  12,   32]]), 2645, 311, 12, 32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "print()\n",
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1200, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 149s - loss: 2.6830 - acc: 0.8285 - f1: 0.8285 - recall: 0.8285 - val_loss: 1.2641 - val_acc: 0.9207 - val_f1: 0.9207 - val_recall: 0.9207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e9c7f8c50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_over, Y_train_over,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20666666666666669"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70454545454545459"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2731,  225],\n",
       "       [  13,   31]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 115s - loss: 1.1101 - acc: 0.9241 - f1: 1.0000 - recall: 1.0000 - val_loss: 1.2822 - val_acc: 0.9177 - val_f1: 1.0000 - val_recall: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e78af8e48>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2722,  234],\n",
       "       [  13,   31]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(256, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(64, input_dim=n_in, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 125s - loss: 1.1800 - acc: 0.9159 - f1: 0.9159 - recall: 0.9159 - val_loss: 1.3593 - val_acc: 0.9147 - val_f1: 0.9147 - val_recall: 0.9147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e4f5e1a90>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2707,  249],\n",
       "        [   7,   37]]), 2707, 249, 7, 37)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22424242424242424, 0.8409090909090909)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tp = 37\n",
    "fn = 7\n",
    "fp = 249\n",
    "tn = 2707\n",
    "r= tp/(tp+fn)\n",
    "p = tp/(tp+fp)\n",
    "f = 2*r*p/(r+p)\n",
    "f, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84090909090909094"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)*np.array([1,1000])\n",
    "recall_score(y_test, A.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 115s - loss: 1.4601 - acc: 0.9025 - f1: 0.9025 - recall: 0.9025 - val_loss: 1.9304 - val_acc: 0.8757 - val_f1: 0.8757 - val_recall: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e4c0c6f28>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2588,  368],\n",
       "        [   5,   39]]), 2588, 368, 5, 39)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17294900221729492"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, A.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88636363636363635"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, A.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 195s - loss: 1.1992 - acc: 0.9201 - f1: 0.9201 - recall: 0.9201 - val_loss: 1.3241 - val_acc: 0.9163 - val_f1: 0.9163 - val_recall: 0.9163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9bde287f60>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(64, input_dim=n_in, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.195718654434\n",
      "recall: 0.727272727273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2705,  251],\n",
       "        [  12,   32]]), 2705, 251, 12, 32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 126s - loss: 1.5779 - acc: 0.8931 - f1: 0.8931 - recall: 0.8931 - val_loss: 1.7485 - val_acc: 0.8897 - val_f1: 0.8897 - val_recall: 0.8897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c97437828>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(512, input_dim=n_in, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(256, input_dim=n_in, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, input_dim=n_in, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.190709046455\n",
      "recall: 0.886363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2630,  326],\n",
       "        [   5,   39]]), 2630, 326, 5, 39)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19070904645476772, 0.8863636363636364)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tp = 39\n",
    "fn = 5\n",
    "fp = 326\n",
    "r= tp/(tp+fn)\n",
    "p = tp/(tp+fp)\n",
    "f = 2*r*p/(r+p)\n",
    "f, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 134s - loss: 2.6317 - acc: 0.8309 - f1: 0.8309 - recall: 0.8309 - val_loss: 3.4076 - val_acc: 0.7840 - val_f1: 0.7840 - val_recall: 0.7840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c2d743b70>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='elu'))\n",
    "model.add(Dense(256, input_dim=n_in, activation='elu'))\n",
    "model.add(Dense(64, input_dim=n_in, activation='elu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_over, Y_train_over,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.107438016529\n",
      "recall: 0.886363636364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2313,  643],\n",
       "        [   5,   39]]), 2313, 643, 5, 39)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 150s - loss: 2.8529 - acc: 0.8141 - f1: 0.8141 - recall: 0.8141 - val_loss: 2.4066 - val_acc: 0.8473 - val_f1: 0.8473 - val_recall: 0.8473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9bde0a5940>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='elu',kernel_initializer='uniform'))\n",
    "model.add(Dense(256, activation='elu', kernel_initializer='uniform'))\n",
    "model.add(Dense(64, activation='elu', kernel_initializer='uniform'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_over, Y_train_over,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.142322097378\n",
      "f0.5: 0.0948103792415\n",
      "f0.5: 0.950634422138\n",
      "recall: 0.863636363636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2504,  452],\n",
       "        [   6,   38]]), 2504, 452, 6, 38)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('f0.5:',fbeta_score(y_test, A.argmax(axis=1), average='binary', beta=0.5))\n",
    "print('f0.5:',fbeta_score(y_test, A.argmax(axis=1), average='weighted', beta=0.5))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 125s - loss: 1.8970 - acc: 0.8702 - f1: 0.8702 - recall: 0.8702 - val_loss: 1.8185 - val_acc: 0.8837 - val_f1: 0.8837 - val_recall: 0.8837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fb1530eb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, input_dim=n_in, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, input_dim=n_in, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy', f1, recall])\n",
    "model.fit(\n",
    "    X_train_over, Y_train_over,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 2],\n",
       "        [1, 1]]), 2, 2, 1, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 0, 0, 1, 0]\n",
    "y_pred = [0, 1, 1, 0, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "confusion_matrix(y_true, y_pred), tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3571428571428571"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tp/(tp+fn)\n",
    "p = tp/(tp+fp)\n",
    "f05 = 1.25*p*r/(0.25*p+r)\n",
    "f05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3571428571428571"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(y_true, y_pred, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15903614457831325"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, A.argmax(axis=1), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.159036144578\n",
      "f0.5: 0.107984293194\n",
      "f0.5: 0.95897787098\n",
      "recall: 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2618,  338],\n",
       "        [  11,   33]]), 2618, 338, 11, 33)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('f0.5:',fbeta_score(y_test, A.argmax(axis=1), average='binary', beta=0.5))\n",
    "print('f0.5:',fbeta_score(y_test, A.argmax(axis=1), average='weighted', beta=0.5))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.92609355668\n"
     ]
    }
   ],
   "source": [
    "print('f1:',f1_score(y_test, A.argmax(axis=1), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 131s - loss: 0.0919 - acc: 0.9724 - val_loss: 0.1113 - val_acc: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f83225cf8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1200, input_dim=n_in, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',#SGD(lr=0.01),\n",
    "    metrics=['accuracy'])#, f1, recall])\n",
    "model.fit(\n",
    "    X_train_over, Y_train_over,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.307692307692\n",
      "recall: 0.318181818182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2923,   33],\n",
       "        [  30,   14]]), 2923, 33, 30, 14)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "13812/13812 [==============================] - 126s - loss: 2.0302 - acc: 0.8636 - val_loss: 2.0921 - val_acc: 0.8670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ed61eefd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling & evaluation\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "n_in = X_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=n_in, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, input_dim=n_in, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, input_dim=n_in, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])#, f1, recall])\n",
    "model.fit(\n",
    "    X_train_over, Y_train_over,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.152866242038\n",
      "recall: 0.818181818182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2565,  391],\n",
       "        [   8,   36]]), 2565, 391, 8, 36)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = model.predict(X_test)#*np.array([1,1])\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, A.argmax(axis=1)).ravel()\n",
    "print('f1:',f1_score(y_test, A.argmax(axis=1)))\n",
    "print('recall:',recall_score(y_test, A.argmax(axis=1)))\n",
    "confusion_matrix(y_test, A.argmax(axis=1)),tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13812 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "13812/13812 [==============================] - 81s - loss: 0.1482 - acc: 0.9555 - val_loss: 0.1306 - val_acc: 0.9433\n",
      "Epoch 2/3\n",
      "13812/13812 [==============================] - 77s - loss: 0.0636 - acc: 0.9791 - val_loss: 0.1025 - val_acc: 0.9550\n",
      "Epoch 3/3\n",
      "13812/13812 [==============================] - 76s - loss: 0.0544 - acc: 0.9810 - val_loss: 0.1258 - val_acc: 0.9523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f608c3f10f0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#171101\n",
    "model.fit(\n",
    "    X_train_smote, Y_train_smote,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71042471042471045"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2837,  119],\n",
       "       [  24,   20]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, np.argmax(model.predict(X_test),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グリッドサーチする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=666)\n",
    "\n",
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train_under, y_train_under = rus.fit_sample(X_train, y_train)\n",
    "X_train_over, y_train_over = ros.fit_sample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp(activation=\"relu\", learning_rate=0.001,\n",
    "        h1=128, h2=128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(h1, input_dim=n_in))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(h2))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"activation\": [\"relu\", \"tanh\", \"sigmoid\", 'elu'],\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01],\n",
    "    \"h1\": [256, 1024, 2048],\n",
    "    \"h2\": [64, 128, 256],\n",
    "    #\"h3\": [8, 32, 64],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_15 to have shape (None, 2) but got array with shape (10359, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2050844d8105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_over\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 638\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/fytroo/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_15 to have shape (None, 2) but got array with shape (10359, 1)"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = KerasClassifier(build_fn=mlp, nb_epoch=50, batch_size=20, verbose=0)\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=4, scoring='f1')\n",
    "res = clf.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = co.med.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
